@inproceedings{chaudhuri2009parameter,
  title={A parameter-free hedging algorithm},
  author={Chaudhuri, Kamalika and Freund, Yoav and Hsu, Daniel J},
  booktitle={Advances in neural information processing systems},
  pages={297--305},
  year={2009}
}

@article{hong2009context,
  title={Context-aware systems: A literature review and classification},
  author={Hong, Jong-yi and Suh, Eui-ho and Kim, Sung-Jin},
  journal={Expert Systems with Applications},
  volume={36},
  number={4},
  pages={8509--8522},
  year={2009},
  publisher={Elsevier}
}

@article{Ao:2016,
 author = {Ao, Buke and Wang, Yongcai and Yu, Lu and Brooks, Richard R. and Iyengar, S. S.},
 title = {On Precision Bound of Distributed Fault-Tolerant Sensor Fusion Algorithms},
 journal = {ACM Comput. Surv.},
 issue_date = {July 2016},
 volume = {49},
 number = {1},
 month = may,
 year = {2016},
 issn = {0360-0300},
 pages = {5:1--5:23},
 articleno = {5},
 numpages = {23},
 url = {http://doi.acm.org/10.1145/2898984},
 doi = {10.1145/2898984},
 acmid = {2898984},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Wireless sensor networks, distributed agreement, sensor fusion},
} 

@INPROCEEDINGS{SongSurvey, 
author={L. Song and Y. Wang and J. J. Yang and J. Li}, 
booktitle={e-Health Networking, Applications and Services (Healthcom), 2014 IEEE 16th International Conference on}, 
title={Health sensing by wearable sensors and mobile phones: A survey}, 
year={2014}, 
pages={453-459}, 
keywords={body sensor networks;geriatrics;health care;information technology;mobile computing;patient treatment;wearable computers;ICT;body sensor networks;elders health care;health sensing technologies;industrialized countries;information and communication technologies;mobile phones;patients health care;population aging;wearable sensors;Accelerometers;Biomedical monitoring;Heart rate;Monitoring;Sensors;Sleep apnea}, 
doi={10.1109/HealthCom.2014.7001885}, 
month={Oct},}


@ARTICLE{6788385, 
author={Baram, Y}, 
journal={Neural Computation}, 
title={Learning by Kernel Polarization}, 
year={2005}, 
volume={17}, 
number={6}, 
pages={1264-1275}, 
doi={10.1162/0899766053630341}, 
ISSN={0899-7667}, 
month={June},}

@article{Gonen:2011,
 author = {G\"{o}nen, Mehmet and Alpaydin, Ethem},
 title = {Multiple Kernel Learning Algorithms},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2211--2268},
 numpages = {58},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021071},
 acmid = {2021071},
 publisher = {JMLR.org},
} 

@article{Lanckriet01112004,
author = {Lanckriet, Gert R. G. and De Bie, Tijl and Cristianini, Nello and Jordan, Michael I. and Noble, William Stafford}, 
title = {A statistical framework for genomic data fusion},
volume = {20}, 
number = {16}, 
pages = {2626-2635}, 
year = {2004}, 
doi = {10.1093/bioinformatics/bth294}, 
abstract ={Motivation: During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data.Results: This paper describes a computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Each dataset is represented via a kernel function, which defines generalized similarity relationships between pairs of entities, such as genes or proteins. The kernel representation is both flexible and efficient, and can be applied to many different types of data. Furthermore, kernel functions derived from different types of data can be combined in a straightforward fashion. Recent advances in the theory of kernel methods have provided efficient algorithms to perform such combinations in a way that minimizes a statistical loss function. These methods exploit semidefinite programming techniques to reduce the problem of finding optimizing kernel combinations to a convex optimization problem. Computational experiments performed using yeast genome-wide datasets, including amino acid sequences, hydropathy profiles, gene expression data and known protein–protein interactions, demonstrate the utility of this approach. A statistical learning algorithm trained from all of these data to recognize particular classes of proteins—membrane proteins and ribosomal proteins—performs significantly better than the same algorithm trained on any single type of data.Availability: Supplementary data at http://noble.gs.washington.edu/proj/sdp-svm}, 
URL = {http://bioinformatics.oxfordjournals.org/content/20/16/2626.abstract}, 
eprint = {http://bioinformatics.oxfordjournals.org/content/20/16/2626.full.pdf+html}, 
journal = {Bioinformatics} 
}


@inproceedings{Rakotomamonjy:2007,
 author = {Rakotomamonjy, Alain and Bach, Francis and Canu, St{\'e}phane and Grandvalet, Yves},
 title = {More Efficiency in Multiple Kernel Learning},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 series = {ICML '07},
 year = {2007},
 isbn = {978-1-59593-793-3},
 location = {Corvalis, Oregon, USA},
 pages = {775--782},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1273496.1273594},
 doi = {10.1145/1273496.1273594},
 acmid = {1273594},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
@article{Slonim:2003,
 author = {Slonim, Noam and Bejerano, Gill and Fine, Shai and Tishby, Naftali},
 title = {Discriminative Feature Selection via Multiclass Variable Memory Markov Model},
 journal = {EURASIP J. Appl. Signal Process.},
 issue_date = {January 2003},
 volume = {2003},
 month = jan,
 year = {2003},
 issn = {1110-8657},
 pages = {93--102},
 numpages = {10},
 url = {http://dx.doi.org/10.1155/S111086570321115X},
 doi = {10.1155/S111086570321115X},
 acmid = {1283271},
 publisher = {Hindawi Publishing Corp.},
 address = {New York, NY, United States},
} 

@incollection{graph,
year={2011},
isbn={978-3-642-20843-0},
booktitle={Graph-Based Representations in Pattern Recognition},
volume={6658},
series={Lecture Notes in Computer Science},
editor={Jiang, Xiaoyi and Ferrer, Miquel and Torsello, Andrea},
doi={10.1007/978-3-642-20844-7_21},
title={A Graph-Based Approach to Feature Selection},
url={http://dx.doi.org/10.1007/978-3-642-20844-7_21},
publisher={Springer Berlin Heidelberg},
author={Zhang, Zhihong and Hancock, EdwinR.},
pages={205-214},
language={English}
}

@article{Wang2015124,
title = "Structural multiple empirical kernel learning ",
journal = "Information Sciences ",
volume = "301",
number = "",
pages = "124 - 140",
year = "2015",
note = "",
issn = "0020-0255",
doi = "http://dx.doi.org/10.1016/j.ins.2015.01.005",
url = "http://www.sciencedirect.com/science/article/pii/S0020025515000250",
author = "Zhe Wang and Qi Fan and Sheng Ke and Daqi Gao",
keywords = "Multiple kernel learning",
keywords = "Empirical kernel mapping",
keywords = "Structural learning",
keywords = "Cluster information",
keywords = "Rademacher complexity",
keywords = "Pattern recognition ",
abstract = "Abstract Multiple Kernel Learning (MKL) can boost classification performance through using multiple kernels rather than a single fixed one. Unlike the traditional \{MKL\} with the implicit kernels, Multiple Empirical Kernel Learning (MEKL) explicitly maps input data into multiple feature spaces. This paper focuses on \{MEKL\} and proposes an effective Threefold Structural \{MEKL\} (TSMEKL). The first fold structure is the space structural information between different mapped feature spaces. The second one is the class discriminant information within each mapped feature space. The third one is the cluster structural information of samples in each mapped feature space. The classical \{MEKL\} mainly pays attention to the first two structures, but neglects the last one. The proposed \{TSMEKL\} introduces the cluster structural information into MEKL. Doing so can simultaneously utilize the space, the class, and the cluster information in the way from globality to locality. Therefore, \{TSMEKL\} utilizes threefold structural information to result in the improvement of classification performance. To the best of our knowledge, it is the first time to introduce the cluster information into the \{MEKL\} framework. The main advantage of the developed \{TSMEKL\} is considering different folds of data information to improve classification performance. The experimental results validate the feasibility and effectiveness of TSMEKL. Moreover, we discuss the theoretical and experimental generalization risk bound of the proposed algorithm in terms of the Rademacher complexity. "
}


@ARTICLE{1593673, 
author={Wei Jiang and Guihua Er and Qionghai Dai and Jinwei Gu}, 
journal={Image Processing, IEEE Transactions on}, 
title={Similarity-based online feature selection in content-based image retrieval}, 
year={2006}, 
volume={15}, 
number={3}, 
pages={702-712}, 
keywords={content-based retrieval;image representation;image retrieval;relevance feedback;content-based image retrieval;image classification;query concept;real-valued fuzzy features;relevance feedback learning process;similarity-based online feature selection;Boosting;Bridges;Content based retrieval;Erbium;Feature extraction;Feedback;Fuzzy sets;Image retrieval;Machine learning;Psychology;Boosting;online feature selection;region-based image retrieval;relevance feedback;Algorithms;Artificial Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Online Systems;Pattern Recognition, Automated;Subtraction Technique}, 
doi={10.1109/TIP.2005.863105}, 
ISSN={1057-7149}, 
month={March},}

@Article{Hoi2012,
author="Hoi, Steven C. H.
and Jin, Rong
and Zhao, Peilin
and Yang, Tianbao",
title="Online Multiple Kernel Classification",
journal="Machine Learning",
year="2012",
volume="90",
number="2",
pages="289--316",
abstract="Although both online learning and kernel learning have been studied extensively in machine learning, there is limited effort in addressing the intersecting research problems of these two important topics. As an attempt to fill the gap, we address a new research problem, termed Online Multiple Kernel Classification (OMKC), which learns a kernel-based prediction function by selecting a subset of predefined kernel functions in an online learning fashion. OMKC is in general more challenging than typical online learning because both the kernel classifiers and the subset of selected kernels are unknown, and more importantly the solutions to the kernel classifiers and their combination weights are correlated. The proposed algorithms are based on the fusion of two online learning algorithms, i.e., the Perceptron algorithm that learns a classifier for a given kernel, and the Hedge algorithm that combines classifiers by linear weights. We develop stochastic selection strategies that randomly select a subset of kernels for combination and model updating, thus improving the learning efficiency. Our empirical study with 15 data sets shows promising performance of the proposed algorithms for OMKC in both learning efficiency and prediction accuracy.",
issn="1573-0565",
doi="10.1007/s10994-012-5319-2",
url="http://dx.doi.org/10.1007/s10994-012-5319-2"
}

@incollection{jin2010,
year={2010},
isbn={978-3-642-16107-0},
booktitle={Algorithmic Learning Theory},
volume={6331},
series={Lecture Notes in Computer Science},
editor={Hutter, Marcus and Stephan, Frank and Vovk, Vladimir and Zeugmann, Thomas},
doi={10.1007/978-3-642-16108-7_31},
title={Online Multiple Kernel Learning: Algorithms and Mistake Bounds},
url={http://dx.doi.org/10.1007/978-3-642-16108-7_31},
publisher={Springer Berlin Heidelberg},
keywords={On-line learning and relative loss bounds; Kernels},
author={Jin, Rong and Hoi, StevenC.H. and Yang, Tianbao},
pages={390-404},
language={English}
}

@INPROCEEDINGS{Luo, 
author={Orabona, F. and Fornoni, M. and Caputo, B. and Cesa-Bianchi, N.}, 
booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on}, 
title={OM-2: An online multi-class Multi-Kernel Learning algorithm Luo Jie}, 
year={2010}, 
pages={43-50}, 
keywords={computer vision;learning (artificial intelligence);OM-2;SILP;SimpleMKL;computer vision;multiclass multikernel learning algorithm;online learning algorithm;visual descriptors;Algorithm design and analysis;Application software;Classification algorithms;Computer vision;Humans;Kernel;Large-scale systems;Learning systems;Support vector machine classification;Support vector machines}, 
doi={10.1109/CVPRW.2010.5543766}, 
ISSN={2160-7508}, 
month={June},}

@ARTICLE{review, 
author={Diethe, T and Girolami, M}, 
journal={Neural Computation}, 
title={Online Learning with (Multiple) Kernels: A Review}, 
year={2013}, 
volume={25}, 
number={3}, 
pages={567-625}, 
doi={10.1162/NECO_a_00406}, 
ISSN={0899-7667}, 
month={March},}

@INPROCEEDINGS{Incremental, 
author={Kembhavi, A. and Siddiquie, B. and Miezianko, Roland and McCloskey, Scott and Davis, L.S.}, 
booktitle={Computer Vision, 2009 IEEE 12th International Conference on}, 
title={Incremental Multiple Kernel Learning for object recognition}, 
year={2009}, 
pages={638-645}, 
keywords={Cameras;Image databases;Kernel;Layout;Lighting;Object recognition;Support vector machine classification;Support vector machines;Vehicles;Visual databases}, 
doi={10.1109/ICCV.2009.5459179}, 
ISSN={1550-5499}, 
month={Sept},}

@inproceedings{martins2011online,
  title={Online Learning of Structured Predictors with Multiple Kernels.},
  author={Martins, Andr{\'e} Filipe Torres and Smith, Noah A and Xing, Eric P and Aguiar, Pedro MQ and Figueiredo, M{\'a}rio AT},
  booktitle={AISTATS},
  pages={507--515},
  year={2011}
}

@article{Foithong2012574,
title = "Feature subset selection wrapper based on mutual information and rough sets ",
journal = "Expert Systems with Applications ",
volume = "39",
number = "1",
pages = "574 - 584",
year = "2012",
note = "",
issn = "0957-4174",
author = "Sombut Foithong and Ouen Pinngern and Boonwat Attachoo",
keywords = "Feature selection",
keywords = "Mutual information",
keywords = "Variable precision rough set model",
keywords = "Multilayer perceptron (MLP) neural networks ",
}

@article{Huang2005325,
title = "Effective feature selection scheme using mutual information ",
journal = "Neurocomputing ",
volume = "63",
number = "",
pages = "325 - 343",
year = "2005",
note = "New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks ",
issn = "0925-2312",
doi = "http://dx.doi.org/10.1016/j.neucom.2004.01.194",
url = "http://www.sciencedirect.com/science/article/pii/S0925231204003285",
author = "D. Huang and Tommy W.S. Chow",
keywords = "Feature selection",
keywords = "Kernel based density estimator",
keywords = "Quadratic mutual information",
keywords = "Supervised data compression ",
abstract = "This article proposes a novel mutual information-based feature selection scheme. In this scheme, the mutual information is estimated directly in an effective way even when one is handling a relative small data set. At the same time, the computation efficiency of the mutual information estimation is improved by proposing a supervised data compression algorithm. With these contributions, the proposed feature selection scheme is able to effectively identify the salience features. The proposed methodology is compared with the related study through applying to different classification problems in which the number of features ranged from less than 10 to over 12,600. The presented results are very promising and corroborate the contributions of the proposed methodology. "
}



@article{Sonnenburg:2006,
 author = {Sonnenburg, S\"{o}ren and R\"{a}tsch, Gunnar and Sch\"{a}fer, Christin and Sch\"{o}lkopf, Bernhard},
 title = {Large Scale Multiple Kernel Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2006},
 volume = {7},
 month = dec,
 year = {2006},
 issn = {1532-4435},
 pages = {1531--1565},
 numpages = {35},
 url = {http://dl.acm.org/citation.cfm?id=1248547.1248604},
 acmid = {1248604},
 publisher = {JMLR.org},
} 


@article{Liu20061333,
title = "FS\_SFS: A novel feature selection method for support vector machines ",
journal = "Pattern Recognition ",
volume = "39",
number = "7",
pages = "1333 - 1345",
year = "2006",
note = "",
issn = "0031-3203",
doi = "http://dx.doi.org/10.1016/j.patcog.2005.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S003132030500378X",
author = "Yi Liu and Yuan F. Zheng",
keywords = "Feature selection",
keywords = "Sequential forward search (SFS)",
keywords = "Support vector machines (SVM) ",
abstract = "In many pattern recognition applications, high-dimensional feature vectors impose a high computational cost as well as the risk of “overfitting”. Feature Selection addresses the dimensionality reduction problem by determining a subset of available features which is most essential for classification. This paper presents a novel feature selection method named filtered and supported sequential forward search (FS_SFS) in the context of support vector machines (SVM). In comparison with conventional wrapper methods that employ the \{SFS\} strategy, FS_SFS has two important properties to reduce the time of computation. First, it dynamically maintains a subset of samples for the training of SVM. Because not all the available samples participate in the training process, the computational cost to obtain a single \{SVM\} classifier is decreased. Secondly, a new criterion, which takes into consideration both the discriminant ability of individual features and the correlation between them, is proposed to effectively filter out nonessential features. As a result, the total number of training is significantly reduced and the overfitting problem is alleviated. The proposed approach is tested on both synthetic and real data to demonstrate its effectiveness and efficiency. "
}




@article{Shen:2008,
 author = {Shen, Kai-Quan and Ong, Chong-Jin and Li, Xiao-Ping and Wilder-Smith, Einar P.},
 title = {Feature Selection via Sensitivity Analysis of SVM Probabilistic Outputs},
 journal = {Mach. Learn.},
 issue_date = {January   2008},
 volume = {70},
 number = {1},
 month = jan,
 year = {2008},
 issn = {0885-6125},
 pages = {1--20},
 numpages = {20},
 url = {http://dx.doi.org/10.1007/s10994-007-5025-7},
 doi = {10.1007/s10994-007-5025-7},
 acmid = {1325288},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Feature ranking, Feature selection, Sensitivity, Support vector machines},
} 


@article{Gu20051,
title = "A service‐oriented middleware for building context‐aware services ",
journal = "Journal of Network and Computer Applications ",
volume = "28",
number = "1",
pages = "1 - 18",
year = "2005",
note = "",
issn = "1084-8045",
doi = "http://dx.doi.org/10.1016/j.jnca.2004.06.002",
url = "http://www.sciencedirect.com/science/article/pii/S1084804504000451",
author = "Tao Gu and Hung Keng Pung and Da Qing Zhang",
keywords = "Context-aware middleware",
keywords = "Pervasive computing",
keywords = "Context-aware services",
keywords = "Network services",
keywords = "Context model",
keywords = "Context ontology ",
abstract = "The advancement of wireless networks and mobile computing necessitates more advanced applications and services to be built with context-awareness enabled and adaptability to their changing contexts. Today, building context-aware services is a complex task due to the lack of an adequate infrastructure support in pervasive computing environments. In this article, we propose a Service-Oriented Context-Aware Middleware (SOCAM) architecture for the building and rapid prototyping of context-aware services. It provides efficient support for acquiring, discovering, interpreting and accessing various contexts to build context-aware services. We also propose a formal context model based on ontology using Web Ontology Language to address issues including semantic representation, context reasoning, context classification and dependency. We describe our context model and the middleware architecture, and present a performance study for our prototype in a smart home environment. "
}



@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

@book{Schrijver:1986:TLI:17634,
 author = {Schrijver, Alexander},
 title = {Theory of Linear and Integer Programming},
 year = {1986},
 isbn = {0-471-90854-1},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
} 

@article{KHACHIYAN198053,
title = "Polynomial algorithms in linear programming",
journal = "USSR Computational Mathematics and Mathematical Physics",
volume = "20",
number = "1",
pages = "53 - 72",
year = "1980",
note = "",
issn = "0041-5553",
doi = "http://dx.doi.org/10.1016/0041-5553(80)90061-0",
url = "http://www.sciencedirect.com/science/article/pii/0041555380900610",
author = "L.G. Khachiyan",
abstract = ""
}

@inproceedings{Karmarkar:1984:NPA:800057.808695,
 author = {Karmarkar, N.},
 title = {A New Polynomial-time Algorithm for Linear Programming},
 booktitle = {Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing},
 series = {STOC '84},
 year = {1984},
 isbn = {0-89791-133-4},
 pages = {302--311},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/800057.808695},
 doi = {10.1145/800057.808695},
 acmid = {808695},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Freund:1999:LMC:337859.337869,
 author = {Freund, Yoav and Schapire, Robert E.},
 title = {Large Margin Classification Using the Perceptron Algorithm},
 journal = {Mach. Learn.},
 issue_date = {Dec. 1999},
 volume = {37},
 number = {3},
 month = dec,
 year = {1999},
 issn = {0885-6125},
 pages = {277--296},
 numpages = {20},
 url = {http://dx.doi.org/10.1023/A:1007662407062},
 doi = {10.1023/A:1007662407062},
 acmid = {337869},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
} 

@article{Freund1997119,
title = "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting ",
journal = "Journal of Computer and System Sciences ",
volume = "55",
number = "1",
pages = "119 - 139",
year = "1997",
note = "",
issn = "0022-0000",
doi = "http://dx.doi.org/10.1006/jcss.1997.1504",
url = "http://www.sciencedirect.com/science/article/pii/S002200009791504X",
author = "Yoav Freund and Robert E Schapire",
abstract = "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line. "
}

@article{bottou1998online,
  title={Online learning and stochastic approximations},
  author={Bottou, L{\'e}on},
  journal={On-line learning in neural networks},
  volume={17},
  number={9},
  pages={142},
  year={1998},
  publisher={Cambridge Univ Pr}
}

@article{penrose1946elementary,
  title={The elementary statistics of majority voting},
  author={Penrose, Lionel S},
  journal={Journal of the Royal Statistical Society},
  volume={109},
  number={1},
  pages={53--57},
  year={1946},
  publisher={JSTOR}
}

@article{lam1997application,
  title={Application of majority voting to pattern recognition: an analysis of its behavior and performance},
  author={Lam, Louisa and Suen, SY},
  journal={IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans},
  volume={27},
  number={5},
  pages={553--568},
  year={1997},
  publisher={IEEE}
}

@inproceedings{wu2009development,
  title={The development and application of decision tree for agriculture data},
  author={Wu, Jun and Olesnikova, Anastasiya and Song, Chi-Hwa and Lee, Won Don},
  booktitle={Intelligent Information Technology and Security Informatics, 2009. IITSI'09. Second International Symposium on},
  pages={16--20},
  year={2009},
  organization={IEEE}
}

@misc{Lichman:2013,
author = "M. Lichman",
year = "2013",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{peterson2009k,
  title={K-nearest neighbor},
  author={Peterson, Leif E},
  journal={Scholarpedia},
  volume={4},
  number={2},
  pages={1883},
  year={2009}
}

@article{liaw2002classification,
  title={Classification and regression by randomForest},
  author={Liaw, Andy and Wiener, Matthew},
  journal={R news},
  volume={2},
  number={3},
  pages={18--22},
  year={2002}
}

@inproceedings{rish2001empirical,
  title={An empirical study of the naive Bayes classifier},
  author={Rish, Irina},
  booktitle={IJCAI 2001 workshop on empirical methods in artificial intelligence},
  volume={3},
  number={22},
  pages={41--46},
  year={2001},
  organization={IBM New York}
}

@article{scholkopft1999fisher,
  title={Fisher discriminant analysis with kernels},
  author={Scholkopft, Bernhard and Mullert, Klaus-Robert},
  journal={Neural networks for signal processing IX},
  volume={1},
  number={1},
  pages={1},
  year={1999}
}

@article{suykens1999least,
  title={Least squares support vector machine classifiers},
  author={Suykens, Johan AK and Vandewalle, Joos},
  journal={Neural processing letters},
  volume={9},
  number={3},
  pages={293--300},
  year={1999},
  publisher={Springer}
}
